---
permalink: /blog/AMP/
share: false
toc: true
toc_label: "Table of Contents"
toc_icon: "cog"
---

# 对抗性运动先验是复杂奖励函数的良好替代

## 概述

**问题**：强化学习中训练一个具有合理可行的高维 Agent 常常需要手工设计复杂的奖励函数，否则由于奖励规范不足容易导致无法在转移到真实世界

**挑战与历史方法**：对于特定问题需要手工设计复杂的奖励函数规范策略行为，并且奖励函数无法容易的在不同平台间迁移；模仿强化学习(基于跟踪)对于单个运动技能的学习很有效，但可能会限制控制器紧跟参考运动，限制智能体拓展到其他任务，并且不同数据集间的模仿学习也很困难；对抗性模仿学习通过模仿参考数据的风格分布，将鉴别器作为风格奖励，在低维域中表现出不错的效果，但在高维连续控制任务中质量远低于 SOTA 的基于跟踪方法

> 评注：这篇文章主要是基于 Amp: Adversarial motion priors for stylized physics-based character control 在四足机器人上的应用

**本文贡献**：
- 引入了一个学习框架，可以使用少量动作捕捉数据编码风格奖励，辅助任务目标一起训练得到可部署在真实机器人上的策略
- 比较了具有手工设计的复杂奖励函数的策略与使用对抗性运动先验训练的策略，发现具有运动先验的策略会产生更自然的步态转换和更节能的运动

## 方法理论

### 对抗性运动先验的风格奖励 Adversarial Motion Priors as Style Rewards

### 动捕数据处理 Motion Capture Data Preprocessing

### 模型表示 Model Representation

