## 使用大规模并行的深度强化学习方法加速行走训练

在单个工作站GPU上使用大规模并行实现对现实世界机器人任务的生成训练

使用大规模并行训练方法，训练设置可以在同一时间框架内频繁迭代(?)

本文研究的问题：

1.  在高度并行的情况下，如何调整标准 RL 公式和常用超参数来有效学习
2.  游戏启发课程 (game-inspired curriculum)

之前的并行化是在不减少每个agent样本量的情况下，通过对不同workers的梯度进行平均实现的，并没有优化整体的训练时间

### 强化学习的大规模并行化

现有强化学习算法分为：数据收集、策略更新

策略更新对应神经网络上的反向传播，易于并行，数据收集较难。

由于当前的模拟、奖励、观察都是在 CPU 上进行，因为 PCIe 通信瓶颈，使得这样的策略推理不适合在 GPU 上进行

<span style="background-color: #ffd40080">通过 Isaac Gym 的端到端数据收集和策略更新政策，能够显著降低数据移动的代价</span>，进而提高仿真的吞吐量

### 仿真吞吐量

影响仿真吞吐量的是仿真中的机器人数量(是通过多机器人同时训练统一策略来提高训练速度吗？)

新的问题：必须使用单一的通用地形网络并且不能在重置时轻易更改(?)

解决方法：创建平铺的所有地形类型与层次网格

### DRL 算法调整

是基于 PPO 算法的自定义实现

所有操作与数据存储都在GPU上进行

#### 超参数调整

相邻两次策略之间的数据收集批大小 batch size 是关键. batch size 过小会导致噪声太大，过大会导致信息重复，降低训练策略

$$
B = n_{robots} * n_{steps}
$$

由于 n\_robots 较大，需要减小 n\_steps，但不能过小(?)，基本不能小于25步对应于0.5s

同时，使用较大的 mini-batch 也具有优势

#### 复位处理

由于 PPO 中包含对于未来无限时域内奖励的预测机制，但在训练中，机器人经常需要重置，会导致对 PPO 算法的挑战。

到达复位时间的复位是无法预测的，采用在超时情况下，使用自举法改进策略(?)

### 任务描述

通过学习，学会跨越具有挑战性的地形

#### Game-Inspired 课程

（什么是粒子滤波方法）

使用复杂性逐步增加的课程训练，当机器人在当前难度上表现较好则提高难度，表现太差回退难度，当达到最高难度后循环回到随机低难度-防止遗忘。可以对每种地形单独调整难度等级

可以直接获得策略性能分布

#### Observations Actions Rewards

Observations: 底盘线速度、角速度，重力矢量，关节位置、速度，先前动作，地形深度测量值

所有地形采用相同奖励的单一策略

Actions: 电机期望关节位置

#### 仿真到实现

仿真中加入摩擦与随机噪声来模拟真实情况

### 结果分析

Batch size 大-训练时间长，reward随机器人数量衰减慢，number of robots在4096时综合训练效果较好

### 结论

可以使用大规模并行的方法在几分钟内训练机器人运动策略，不需要鼓励特定步态
可以用于其他训练中提高效率，尤其对于默写实际应用的训练用时。