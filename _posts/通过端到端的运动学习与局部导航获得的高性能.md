# 通过端到端的运动学习与局部导航获得的高性能

大多数相关工作(2022 速度跟踪和连续位置方法(?))将足式机器人的局部导航任务拆分为多个子任务，各个子任务中通过速度追踪完成运动控制

在基于模型的和学习运动的控制器中，速度追踪都是很重要的任务

这种方式限制了从整体上得到更优解的可能性，并且以大多数步态都为对角小步跑

本文的方法可以在达到目标点的情况下，自由的调整步态和其轨迹，并且不需要局部路径规划器。

## Method

使用 Deep RL 对神经网络进行仿真训练，使用Iscaac Gym模拟环境，以PPO算法作为深度强化学习算法。神经网络以机器人传感器数据作为输入

PPO 算法各项内容

1.  observation: 关节位置，关节速度。基座线速度和角速度，指令(目的地的三维位置和)，先前动作及周围地形采样

2.  noise: 基于传感器的噪声水平给出

3.  actions: 给电机的目标关节位置

4.  rewards: 由奖励和惩罚构成：

    1.  任务 reward : 在不超过最大可接受时间且大于一定时间T\_r（防止无法收敛到目标位置）的时间下，与位置偏差的平方成反比(**为什么这个奖励被设置在时间片将要结束的时候显现？(t = T - T\_r)，是让其选择运动轨迹更自由吗**)
    2.  惩罚 penalties ：对于关节加速度，关节力矩，碰撞次数，相邻两次动作的相对幅度，脚的加速度设计惩罚
    3.  exploration reward : 由于 reward 的稀疏性，在开始训练时可以增加额外的奖励鼓励向目标前进。随着训练进行，该项奖励被移除，因此对最终解并不约束
    4.  stall penalty (等待惩罚) : PPO 优化了训练过程中的奖励总和的折扣。为了防止这种折扣带来的最后一刻的突然行动，需要在”未来”（在目标远处等待时）增加小的等待惩罚

环境地形：

1.  gap: 必须跳过一个gap
2.  沟：必须从一个很深的沟爬上去
3.  obstacle: 必须学会绕过障碍

训练稳定性

由于reward的时间上的稀疏性，导致难以训练出正确的策略

解决方法：

1.  增加策略迭代的步数

## 仿真表现

与速度追踪方法相比，可以获得更多步态，并且能处理更复杂的地形

## 能量消耗

与速度追踪等方法相比，在低速情况下，能耗更优，高速时，收敛到类似程度

## 步态

相较速度追踪，更自然，但是倾向于转弯而不是后退或侧行

## 硬件部署

总体来说，新的训练策略相较于传统的速度追踪等方法，能够展现更强的运动能力，对于传统方法不能解决的地形有更好的表现。但在高速情况下，策略会选择高转矩，有悖于无刷电机的工作原理(?)。并且在跳跃或攀爬情况下，成功率较低，可能是由于感知模块的设计局限

## 展望

本项目提出了一种训练四足机器人的运动策略的新方法。

优点：能够支持机器人在轨迹规划和步态选择上更自由，从整体上考虑可以拓展最优解空间

缺点：训练方法可能不够稳定，没有利用运动学对称性(?)

应用与未来方向：

1.  可以扩展到连续的多个路径点；
2.  感知模块的改进；
3.  对于轨迹的自由选择可以用于多种任务
